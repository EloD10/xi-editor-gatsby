{"data":{"markdownRemark":{"id":"4dd8698d-d45f-543c-9905-eba9659b68b0","html":"<p><em>23 Apr 2017</em></p>\n<p>In this post, we present an incremental algorithm for syntax\nhighlighting. It has very good performance, measured primarily by\nlatency but also memory usage and power consumption. It does not\nrequire a large amount of code, but the analysis is subtle and\nsophisticated. Your favorite code editor would almost certainly\nbenefit from adopting it.</p>\n<p>Pedagogically, this post also gives a case study in systematically\ntransforming a simple functional program into an incremental\nalgorithm, meaning an algorithm that takes a delta on input and\nproduces a delta on output, so that applying that delta gives the same\nresult as running the entire function from scratch, beginning to end.\nSuch algorithms are the backbone of xi editor, the basis of\nnear-instant response even for very large files.</p>\n<h2 id=\"the-syntax-highlighting-function\"><a href=\"#the-syntax-highlighting-function\" aria-hidden class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>The syntax highlighting function</h2>\n<p>Most syntax highlighting schemes (including the TextMate/Sublime/Atom\nformat) follow this function signature for the basic syntax\nhighlighting operation (code is in pseudo-rust):</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">fn syntax (previous_state, line) -&gt; (next_state, spans);</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span></span></pre></div>\n<p>Typically this \"state\" is a stack of finite states, i.e. this is a\n<a href=\"https://en.wikipedia.org/wiki/Pushdown_automaton\">pushdown\nautomaton</a>. Such\nautomata can express a large family of grammars. In fact, the\nincredibly general class of <a href=\"https://en.wikipedia.org/wiki/LR_parser\">LR\nparsers</a> could be accomodated\nby adding one additional token of lookahead in addition to the line,\na fairly straightforward extension to this algorithm.</p>\n<p>I won't go into more detail about the syntax function itself; within\nthis framework, the algorithms described in this post are entirely\ngeneric.</p>\n<h2 id=\"a-batch-algorithm\"><a href=\"#a-batch-algorithm\" aria-hidden class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>A batch algorithm</h2>\n<p>The simplest algorithm to apply syntax highlighting to a file is to\nrun the function on each line from beginning to end:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">let mut state = State::initial();\nfor line in input_file.lines() {\n    let (new_state, spans) = syntax(state, line);\n    output.render(line, spans);\n    state = new_state;\n}</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<p>This algorithm has some appealing properties. In addition to being\nquite simple, it also has minimal memory requirements: one line of\ntext, plus whatever state is required by the syntax function. It's\nuseful for highlighting a file on initial load, and also for\napplications such as statically generating documentation files.</p>\n<p>For this post, it's also something of a correctness spec; all the\nfancy stuff we do has to give the same answer in the end.</p>\n<h2 id=\"random-access-caching\"><a href=\"#random-access-caching\" aria-hidden class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Random access; caching</h2>\n<p>Let's say we're not processing the file in batch mode, but will be\ndisplaying it in a window with ability to scroll to a random point,\nand want to be able to compute the highlighting on the fly. In\nparticular, let's say we don't want to store all the spans for the\nwhole file. Even in a compact representation, such spans are\ncomparable to the size of the input text, potentially much more.</p>\n<p>We can write the following functional program:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">fn get_state(file, line_number) -&gt; state {\n    file.iter_lines(0, line_number).fold(\n        State::initial(),\n        |state, line| syntax(state, line).state\n    )\n}\n\nfn get_spans(file, line_number) -&gt; spans {\n    let state = get_state(file, line_number);\n    syntax(state, file.get_line(line_number)).spans\n}</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<p>This will work very well for lines near the beginning of the file,\nbut has a serious performance problem; it is O(n) to retrieve one\nline's worth of spans, so O(n^2) to process the file.</p>\n<p>Fortunately, <a href=\"https://en.wikipedia.org/wiki/Memoization\">memoization</a>,\na traditional technique for optimizing functional programs, can come\nto the rescue. Storing the intermediate results of <code>get_state</code> reduces\nthe runtime back to O(n). We also see the algorithm start to become\nincremental, in that it's possible to render the first screen of the\nfile quickly, without having to process the whole thing.</p>\n<p>However, these benefits come at a cost, namely the memory required\nto store the intermediate results. In this case, we only need store\nthe state per line (which, in a compact representation, need only be\none machine word), so it might be acceptable. But to handle extremely\nlarge files, we might want to do better.</p>\n<p>One good compromise would be to use a <em>cache</em> with only partial\ncoverage of the <code>get_state</code> function; when the cache overflows, we\nevict some entry in the cache to make room. Then, to compute\n<code>get_state</code> for an arbitrary line, we find closest previous cache\nentry, and run the fold forward from there.</p>\n<p>This cache is a classic speed/space tradeoff. The amount of time to\ncompute a query is essentially proportional to the <em>gap length</em>\nbetween one entry and the next. For random access patterns, it follows\nthat the optimal pattern would be evenly spaced entries. Then the\ntime required for a query is O(n/m), where m is the cache size.</p>\n<p>Tuning such a cache, in particular choosing a cache replacement\nstrategy, is tricky. We'll defer discussion of that for later.</p>\n<h2 id=\"handling-mutation\"><a href=\"#handling-mutation\" aria-hidden class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Handling mutation</h2>\n<p>Of course, we <em>really</em> want to be able to do interactive syntax\nhighlighting on a file being edited. Fortunately, the above cache can\nbe extended to handle this use case as well.</p>\n<p>As the file is mutated, existing cache entries might become <em>invalid.</em>\nWe define a cache entry (line<em>number, state) as being _valid</em> if that\nstate is actually equal to computing <code>get_state(line_number)</code> from\nscratch. Editing a line need not only change the spans for that line;\nit might cause state changes that ripple down from there. A classic\nexample would be inserting <code>/*</code> to open a comment; then the entire\nrest of the file would be rendered as a comment. So, unlike a typical\ncache, changing one line might invalidate an arbitrary fraction of the\ncache contents.</p>\n<p>We augment the cache with a <em>frontier,</em> a set of cache entries. All\noperations maintain the following invariant:</p>\n<p><em>If a cache entry is valid and it is not in the frontier, then the\nnext entry in the cache is also valid.</em></p>\n<p>From this invariant immediately follows a number of useful properties.\nAll lines up to the first element of the frontier are valid. Thus, if\nthe frontier is empty, the entire cache is valid.</p>\n<p>This invariant is carefully designed so that it can be easily restored\nafter an editing operation, specifically that all operations take\nminimal time (I <em>think</em> it's O(1) amortized, but establishing that\nwould take careful analysis).</p>\n<p>Specifically, after changing the contents of a single line, it\nsuffices to add the closest previous cache entry to the frontier.\nOther editing operations are similarly easy; to replace an arbitrary\nregion of text, also delete cache entries for which the starts of the\nlines are in strictly in the interior of the region. For inserts and\ndeletes, the line numbers after the edit will also need to be fixed\nup.</p>\n<p>Of course, it's not enough to properly invalidate the cache, it's also\nimportant to make progress towards re-validating it. Here is the\nalgorithm to do one granule of work:</p>\n<ul>\n<li>Take the first element of the frontier. It refers to a cache entry:\n<code>(line_number, state)</code>.</li>\n<li>Evaluate <code>syntax(state, file.get_line(line_number))</code>, resulting in a\nnew_state.</li>\n<li>If <code>line_number + 1</code> does not have an entry in the cache, or if it\ndoes and the entry's state != new<em>state, then insert\n`(line</em>number + 1, new_state)` into the cache, and move this element\nof the frontier to that entry.</li>\n<li>Otherwise, just delete this element from the frontier.</li>\n</ul>\n<p>The only other subtle operation is deleting an entry from the cache\n(especially evictions). If that entry is in the frontier, then the\nelement of the frontier must be moved to the previous entry.</p>\n<h2 id=\"on-the-representation-of-the-frontier\"><a href=\"#on-the-representation-of-the-frontier\" aria-hidden class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>On the representation of the frontier</h2>\n<p>It's tempting to truncate the frontier, rather than storing it as a\nset. In particular, it's perfectly correct to just store it as a\nreference to the first entry. Then, the operation of adding an element\nto the frontier reduces to just taking the minimum.</p>\n<p>However, this temptation should be resisted. Let's say the user opens\na comment at the beginning of a large file. The frontier slowly\nripples through the file, recomputing highlighting so that all lines\nare in a \"commented\" state. Then say the user closes the comment when\nthe frontier is about halfway through the file. This edit will cause\na new frontier to ripple down, restoring the uncommented state. With\nthe full set representation of the frontier, the old position halfway\nthrough the file will be retained, and when the new frontier reaches\nit, states will match, so processing can stop.</p>\n<p>If that old position were not retained, then the frontier would need\nto ripple all the way to the end of the file before there would be\nconfidence the entire cache was valid. So, for a relatively small cost\nof maintaining the frontier as a set, we get a pretty nice\noptimization, which will improve power consumption and also latency\n(the editor can respond more quickly when it has quiesced as opposed\nto doing computation in the background).</p>\n<h2 id=\"tuning-the-cache\"><a href=\"#tuning-the-cache\" aria-hidden class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Tuning the cache</h2>\n<p>This is where the rocket science starts. Please check your flight\nharnesses.</p>\n<h3 id=\"access-patterns\"><a href=\"#access-patterns\" aria-hidden class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Access patterns</h3>\n<p>Before we can start tuning the cache, we have to characterize the\naccess patterns. In an interactive editing session, the workload will\nconsist of a mix of three fundamental patterns: sequential, local, and\nrandom.</p>\n<p>Sequential is familiar from the first algorithm we presented. It's an\nimportant case when first loading a file. It will also happen when\nedits (such as changing comment balance) cause state changes to ripple\nthrough the file. The cache is basically irrelevant to this access\npattern; the computation has to happen in any case, so the only\npurpose of the cache is not to have significant overhead.</p>\n<p>By \"local,\" we mean edits within a small region of the file, typically\naround one screenful. Most such edits <em>won't</em> cause extensive state\nchanges, in fact should result in re-highlighting of just a line or\ntwo. In this access pattern, we want our algorithm to recompute tiny\ndeltas, so the cache should be <em>dense,</em> meaning that the gap between\nthe closest previous cache entry and the line being edited be zero or\nvery small.</p>\n<p>The random access pattern is the most difficult for a cache to deal\nwith. The best we can possibly do is O(n/m), as above. We expect these\ncases to be rare compared with the other two, but it is still\nimportant to have reasonable worst-case behavior.</p>\n<p>Any given editing session will consist of all three of these patterns,\ninterleaved, in some relative proportions. This is significant for\ndesigning a well-tuned cache, especially because processing some work\nfrom one pattern may leave the cache in poor condition for the next.</p>\n<h3 id=\"analyzing-the-cache-performance\"><a href=\"#analyzing-the-cache-performance\" aria-hidden class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Analyzing the cache performance</h3>\n<p>In most applications, cache performance is characterized almost\nentirely by its <em>hit rate,</em> meaning the probability that any given\nquery will be present in the cache. Most <a href=\"https://en.wikipedia.org/wiki/Cache_replacement_policies\">cache eviction\npolicies</a>\nare chosen to optimize this quantity.</p>\n<p>However, for this algorithm, the cost of a cache miss is highly\ndependent on the <em>gap</em> between entries, and the goal should be to\nminimize this gap.</p>\n<p>From this viewpoint, we can see that the LRU (least recently used)\npolicy, while fine for local access patterns, is absolutely worst case\nwhen mixing sequential with anything else; after sequential procesing,\nthe cache will consist of a dense block (often at the end of the\nfile), with a huge gap between the beginning of the file and that\nblock. As Dan Luu's excellent <a href=\"http://danluu.com/2choices-eviction/\">case\nstudy</a> points out, LRU can also\nhave this kind of pathological performance in more traditional\napplications such as memory hierarchies.</p>\n<p>For the \"random\" access pattern, the metric we care about is maximum\ngap; this establishes a worst case. For LRU, it is O(n), which is\nterrible. We want to do better.</p>\n<p>The obvious next eviction policy candidate to consider is randomized.\nIn traditional cache applications, random eviction fixes the pathology\nwith perfectly sequential workloads, and performs reasonably well\noverall (in Dan's analysis, it is better than LRU for some real-world\nworkloads, worse in others, and in no case has a hit rate more than\nabout 10% different).</p>\n<p>I tried simulating it [TODO: a more polished version of this document</p>\n<p>would contain lots of beautiful visualizations, plus a cleaned up</p>\n<p>version of the simulation code], and the maximum-gap metric was\nhorrible, almost as bad as it can get. In scanning the file from\nbeginning to end, in the final state the entries near the beginning\nare decimated; a typical result is that the first entry remaining in\nthe cache is about halfway through the file.</p>\n<p>For a purely random workload, an ideal replacement policy would be to\nchoose the entry with the smallest gap between previous and next\nentries. A bit of analysis shows that this policy would yield a\nmaximum gap of 2n/m in the worst case. However, it won't perform well\nfor local access patterns - basically, the state of the cache will\nbecome stuck, as lines most recently added are likely to also have the\nsmallest gap. Thus, local edits will still have a cost around n/m\nlines re-highlighted. It doesn't make sense to optimize for the random\ncase at the expense of the local one.</p>\n<p>Inspired by Dan's post, I sought a hybrid. My proposed cache eviction\npolicy is to probe some small number k of random candidates, and of\nthose choose the one with the smallest gap as defined above. In my\nsimulations [TODO: I know, this really needs graphs; what I have now</p>\n<p>is too rough], it performs <em>excellently.</em></p>\n<p>There's no obvious best choice of k, it's a tradeoff between the\nexpected mix of local (where smaller is better) and random (where\nlarger is better). However, there seems to be a magic threshold of 5;\nfor any smaller value, the maximum gap grows very quickly with the\nfile size, but for 5 or larger it levels off. In a simulation of an\n8k entry cache and a sequential scan through an 8M line file, k=5\nyielded a maximum gap of ~9k lines (keep in mind that 2k is the best\npossible result here). Beyond that, increasing k doesn't have dramatic\nconsequences, even at k=10 this metric improves only to ~3600, and\nthat's at the expense of degrading local access patterns.</p>\n<p>Obviously it's possible to do a more rigorous analysis and more\nfine-tuning, but my gut feeling is that this very simple policy will\nperform within a small factor of anything more sophisticated; I'd be\nshocked if any policy could improve performance more than a doubling\nof the cache size, and with the cache sizes I have in mind, that\nshould be well affordable. And a larger cache size always has the\nadvantage that any file with a number of lines that fits entirely\nwithin the cache will have perfect effectiveness.</p>\n<h3 id=\"cache-size-and-representation\"><a href=\"#cache-size-and-representation\" aria-hidden class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Cache size and representation</h3>\n<p>Choosing cache size is always a tradeoff between cache effectiveness\n(whether hit rate or maximum-gap) and the cost of the cache itself.\nA larger cache should increase effectiveness, but how much?</p>\n<p>This is an empirical question, but we can try to analyze it. Cache\neffectiveness is irrelevant for sequential access. For the local case,\nit would be reasonable to expect that the \"working set\" is quite\nsmall, typically on the order of 1000 lines or so.</p>\n<p>And for the random case, the cache only has to perform reasonably\nwell; we expect these cases to be rare.</p>\n<p>From this, we can guess that the cache doesn't have to be very large\nto be effective. Thus, a very simple representation is a dense vector\nof entries. Some operations (such as deletion and fixup of line\nnumbers) are O(m) in the size of the cache, but with a very good\nconstant factor due to the vector representation. So, while it's\ntempting to use a fancy O(log m) data structure such as a B-tree, this\nis probably a case where simpler is better.</p>\n<p>My gut feeling is that a fixed maximum size of 10k entries will yield\nnear-optimal results in all cases.</p>\n<h3 id=\"implementation-state-and-summary\"><a href=\"#implementation-state-and-summary\" aria-hidden class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Implementation state and summary</h3>\n<p>I haven't implemented this yet (beyond the simulations), but really\nlook forward to it.</p>\n<p>Based on my analysis, this algorithm should provide truly excellent\nperformance, producing minimal deltas with very modest memory\nrequirements. I'm also pleased that the code and data structures are\nrelatively easy; I have considered <em>much</em> more sophisticated\napproaches (including of course my beloved balanced-tree\nrepresentation for the cache), which in analysis wouldn't perform\nnearly as well.</p>\n<p>I think it would be interesting to do a more rigorous analysis. It's\npossible this technique has already been investigated somewhere, but\nI'm not aware of it; I'd <em>love</em> to find such a literature.</p>\n<p>Thanks to <a href=\"https://github.com/cmyr\">Colin Rofls</a> for stimulating\ndiscussions about caching in plugins that inspired many of these\nideas.</p>","frontmatter":{"title":"Rope science, part 11 - practical syntax highlighting"}},"site":{"siteMetadata":{"title":"Gatsby Blog Tailwindcss","titleSeparator":"|"}},"allMarkdownRemark":{"edges":[{"node":{"id":"159ebaf9-aed2-5ad5-b403-f0a2c391cbef","fields":{"slug":"/frontend-notes/"},"frontmatter":{"title":"Notes on writing front-ends"}}},{"node":{"id":"db31e77f-3fda-57fc-9830-61c9f7c222dd","fields":{"slug":"/frontend-protocol/"},"frontmatter":{"title":"The Frontend Protocol"}}},{"node":{"id":"e81dd02d-d53d-5928-978d-37b7a16cdb61","fields":{"slug":"/plugin/"},"frontmatter":{"title":"Plugin architecture"}}},{"node":{"id":"056635a4-9cff-573b-9253-1c2afd68ff96","fields":{"slug":"/config/"},"frontmatter":{"title":"Working with the config system"}}},{"node":{"id":"ef83d327-af33-537c-b865-304c9cbd7567","fields":{"slug":"/crdt/"},"frontmatter":{"title":"CRDT - An approach to async plugins and undo"}}},{"node":{"id":"e7b10047-5613-53ea-bdeb-3989cfe8ac10","fields":{"slug":"/crdt-details/"},"frontmatter":{"title":"CRDT - The Xi Text Engine"}}},{"node":{"id":"8493f667-6117-50a0-bc92-21f304190a9f","fields":{"slug":"/fuchsia-ledger-crdts/"},"frontmatter":{"title":"CRDT - Using the Ledger for CRDTs"}}},{"node":{"id":"637adc80-f545-583f-b103-0e6256f0efe1","fields":{"slug":"/rope-science-00/"},"frontmatter":{"title":"Rope science - Introduction"}}},{"node":{"id":"d6abbbb0-ce0b-5810-8ab1-40db5f8fbb4d","fields":{"slug":"/rope-science-01/"},"frontmatter":{"title":"Rope science, part 1 - MapReduce for text"}}},{"node":{"id":"0b56a298-7fea-52be-b0c1-1924e8de94df","fields":{"slug":"/rope-science-02/"},"frontmatter":{"title":"Rope science, part 2 - metrics"}}},{"node":{"id":"f3d25a72-d849-52f7-8fb4-6d586db9b401","fields":{"slug":"/rope-science-03/"},"frontmatter":{"title":"Rope science, part 3 - Grapheme cluster boundaries"}}},{"node":{"id":"ed9dc1bf-9c35-58a0-b8ed-e702bdbc974f","fields":{"slug":"/rope-science-04/"},"frontmatter":{"title":"Rope science, part 4 - parenthesis matching"}}},{"node":{"id":"e3b3971e-a314-5c04-adc0-30c2e2f21345","fields":{"slug":"/rope-science-05/"},"frontmatter":{"title":"Rope science, part 5 - incremental word wrapping"}}},{"node":{"id":"271d1a9b-90e4-5315-b138-2a02a017e117","fields":{"slug":"/rope-science-06/"},"frontmatter":{"title":"Rope science, part 6 - parallel and asynchronous word wrapping"}}},{"node":{"id":"ef5e3b38-01da-5a9c-ad36-fce16dacff86","fields":{"slug":"/rope-science-08/"},"frontmatter":{"title":"Rope science, part 8 - CRDTs for concurrent editing"}}},{"node":{"id":"b73f0ca0-d039-531c-91a7-b8d0893fc5e8","fields":{"slug":"/rope-science-08a/"},"frontmatter":{"title":"Rope science, part 8a - CRDT follow-up"}}},{"node":{"id":"0c5f285f-2a9b-589f-8eff-a79435e54257","fields":{"slug":"/rope-science-09/"},"frontmatter":{"title":"Rope science, part 9 - CRDT Approach to Async Plugins and Undo"}}},{"node":{"id":"ef95af30-321f-5d87-a5e3-720e4eff00f9","fields":{"slug":"/rope-science-10/"},"frontmatter":{"title":"Rope science, part 10 - designing for a conflict-free world"}}},{"node":{"id":"4dd8698d-d45f-543c-9905-eba9659b68b0","fields":{"slug":"/rope-science-11/"},"frontmatter":{"title":"Rope science, part 11 - practical syntax highlighting"}}},{"node":{"id":"c8b885e6-57e4-5e66-9651-7b47f528793d","fields":{"slug":"/rope-science-12/"},"frontmatter":{"title":"Rope science, part 12 - minimal invalidation"}}}]}},"pageContext":{"id":"4dd8698d-d45f-543c-9905-eba9659b68b0"}}